---
title: "R Notebook"
output: html_notebook
---
Let us have a look at an example where using a nonlinear regression function is
better suited for estimating the population relationship between the regressor,
X, and the regressand, Y : the relationship between the income of schooling
districts and their test scores.
```{r}
library(AER)
library(stargazer)
data(CASchools)
```

```{r}
CASchools$size <- CASchools$students/CASchools$teachers
CASchools$score <- (CASchools$read + CASchools$math) / 2
```
We start our analysis by computing the correlation between both variables.
```{r}
cor(CASchools$income, CASchools$score)
```
Here, income and test scores are positively related: school districts with above
average income tend to achieve above average test scores. Does a linear regression
function model the data adequately? Let us plot the data and add a linear
regression line.
```{r}
# fit a simple linear model
linear_model<- lm(score ~ income, data = CASchools)
# plot the observations
plot(CASchools$income, CASchools$score,
col = "steelblue",
pch = 20,
xlab = "District Income (thousands of dollars)",
ylab = "Test Score",
cex.main = 0.9,
main = "Test Score vs. District Income and a Linear OLS Regression Function")
# add the regression line to the plot
abline(linear_model,
col = "red",
lwd = 2)
```
As pointed out in the book, the linear regression line seems to overestimate the
true relationship when income is very high or very low and underestimates it
for the middle income group.

TestScorei = B0 + B1 × incomei + B2 × income2i + ui called a quadratic regression model.That is, income2 is treated as an additional
explanatory variable. Hence, the quadratic model is a special case of a
multivariate regression model.

```{r}
# fit the quadratic Model
quadratic_model <- lm(score ~ income + I(income^2), data = CASchools)
# obtain the model summary
coeftest(quadratic_model, vcov. = vcovHC, type = "HC1")
```
```{r}
print(summary(quadratic_model))
```
This model allows us to test the hypothesis that the relationship between test
scores and district income is linear against the alternative that it is quadratic.
H0 : B2 = 0 vs. H1 : B2 != 0
since B2 = 0 corresponds to a simple linear equation and B2 != 0 implies a
quadratic relationship. We find that t = ( B2− 0)/SE( SE(B2) = −0.0423/0.0048 =
−8.81 so the null is rejected at any common level of significance and we conclude
that the relationship is nonlinear. This is consistent with the impression gained
from the plot.

```{r}
#draw a scatterplot of the observations for income and test score
plot(CASchools$income, CASchools$score,
col = "steelblue",
pch = 20,
xlab = "District Income (thousands of dollars)",
ylab = "Test Score",
main = "Estimated Linear and Quadratic Regression Functions")

# add a linear function to the plot
abline(linear_model, col = "black", lwd = 2)

# add quadratic function to the plot
order_id <- order(CASchools$income)
lines(x = CASchools$income[order_id],
y = fitted(quadratic_model)[order_id],
col = "red",
lwd = 2)
```
We see that the quadratic function does fit the data much better than the linear
function.

8.2 Nonlinear Functions of a Single Independent Variable

```{r}
# estimate a cubic model
cubic_model <- lm(score ~ poly(income, degree = 3, raw = TRUE), data = CASchools)
print(summary(cubic_model))
```
In practice the question will arise which polynomial order should be chosen.
First, similarly as for r = 2, we can test the null hypothesis that the true
relation is linear against the alternative hypothesis that the relationship is a
polynomial of degree r:


H0 : B2 = 0, B3 = 0, . . . , Br = 0 vs. H1 : at least one Bj != 0, j = 2, . . . , r


This is a joint null hypothesis with r−1 restrictions so it can be tested using the
F-test presented in Chapter 7. linearHypothesis() can be used to conduct
such tests. For example, we may test the null of a linear model against the
alternative of a polynomial of a maximal degree r = 3 as follows.

```{r}
# test the hypothesis of a linear model against quadratic or polynomial alternatives

# set up hypothesis matrix
R <- rbind(c(0, 0, 1, 0),c(0, 0, 0, 1))

# do the test
linearHypothesis(cubic_model,
hypothesis.matrix = R,
white.adj = "hc1")
```
NULL and ALTERNATE HYPOTHESIS

H0 = 0*B0 + 0*B1 + 0*B2 + 1B3 +0B4
Ha = 0*B0 + 0*B1 + 0*B2 + 0B3 +1B4

R = ((0, 0, 1, 0)
    (0, 0, 0, 1))

We provide a hypothesis matrix as the argument hypothesis.matrix. This is useful when the coefficients have long names, as is the case here due to using poly(), or when the restrictions include multiple coefficients.
For the two linear constrains above, we have RB =s

The p-value for is very small so that we reject the null hypothesis. However, this does not tell us which r to choose. In practice, one approach to determine the degree of the polynomial is to use sequential testing:

1. Estimate a polynomial model for some maximum value r.
2. Use a t-test to test Br = 0. Rejection of the null means that Xr belongs
in the regression equation.
3. Acceptance of the null in step 2 means that Xr can be eliminated from the
model. Continue by repeating step 1 with order r − 1 and test whether
Br−1 = 0. If the test rejects, use a polynomial model of order r − 1.
4. If the tests from step 3 rejects, continue with the procedure until the
coefficient on the highest power is statistically significant.
```{r}
summary(cubic_model)
```
TestScorei = 600.1+ 5.02(income) − 0.96(income^2 )− 0.00069(income3).
The t-statistic on income3 is 1.42 so the null that the relationship is quadratic
cannot be rejected, even at the 10% level. This is contrary to the result presented
book which reports robust standard errors throughout so we will also use robust
variance-covariance estimation to reproduce these results.

```{r}
# test the hypothesis using robust standard errors
coeftest(cubic_model, vcov. = vcovHC, type = "HC1")
```
The reported standard errors have changed. Furthermore, the coefficient for
incomeˆ3 is now significant at the 5% level. This means we reject the hypothesis that the regression function is quadratic against the alternative that it is cubic.Furthermore, we can also test if the coefficients for income^2 and income^3 are jointly significant using a robust version of the F-test.

```{r}
# perform robust F-test
linearHypothesis(cubic_model,
hypothesis.matrix = R,
vcov. = vcovHC, type = "HC1")
```
With a p-value of 9.043e−16, i.e., much less than 0.05, the null hypothesis of linearity is rejected in favor of the alternative that the relationship is quadratic or cubic.

Coeffiecients and their interpretations

```{r}
# compute and assign the quadratic model
quadriatic_model <- lm(score ~ income + I(income^2), data = CASchools)
# set up data for prediction
new_data <- data.frame(income = c(10, 11))
# do the prediction
Y_hat <- predict(quadriatic_model, newdata = new_data)
# compute the difference
diff(Y_hat)
```

```{r}
# set up data for prediction
new_data <- data.frame(income = c(40, 41))
# do the prediction
Y_hat <- predict(quadriatic_model, newdata = new_data)
# compute the difference
diff(Y_hat)
```
So for the quadratic model, the expected change in TestScore induced by an
increase in income from 10 to 11 is about 2.96 points but an increase in income
from 40 to 41 increases the predicted score by only 0.42. Hence, the slope of the estimated quadratic regression function is steeper at low levels of income than at higher levels.

Logarithms

Case I: X is in Logarithm, Y is not (lin-log model)
```{r}
# estimate a level-log model
LinearLog_model <- lm(score ~ log(income), data = CASchools)
# compute robust summary
coeftest(LinearLog_model,
vcov = vcovHC, type = "HC1")
```
The estimated function is : ^TestScore = 557.8 + 36.42 × ln(income).
```{r}
# draw a scatterplot
plot(score ~ income,
col = "steelblue",
pch = 20,
data = CASchools,
main = "Linear-Log Regression Line")
# add the linear-log regression line
order_id <- order(CASchools$income)
lines(CASchools$income[order_id],
    fitted(LinearLog_model)[order_id],
    col = "red",
    lwd = 2)
```
We can interpret B1 as follows: a 1% increase in income is associated with an
increase in test scores of 0.01×36.42 = 0.36 points.
In order to get the estimated effect of a one unit change in income (that is, a change in the original units,
thousands of dollars) on test scores, the method presented in Key Concept 8.1
can be used.

```{r}
# set up new data
new_data <- data.frame(income = c(10, 11, 40, 41))
# predict the outcomes
Y_hat <- predict(LinearLog_model, newdata = new_data)
# compute the expected difference
Y_hat_matrix <- matrix(Y_hat, nrow = 2, byrow = TRUE)
Y_hat_matrix[, 2] - Y_hat_matrix[, 1]
```
By setting nrow = 2 and byrow = TRUE in matrix() we ensure that
Y_hat_matrix is a 2 × 2 matrix filled row-wise with the entries of Y_hat.
Interpretation: The estimated model states that for an income increase from $10000 to $11000,test scores increase by an expected amount of 3.47 points.When income increases from $40000 to $41000, the expected increase in test scores is only about
0.90 points.


Case II: Y is in Logarithm, X is not
```{r}
# estimate a log-linear model
LogLinear_model <- lm(log(score) ~ income, data = CASchools)
# obtain a robust coefficient summary
coeftest(LogLinear_model,
vcov = vcovHC, type = "HC1")
```
An increase in district income by $1000 is expected to increase test scores by
100 × 0.00284% = 0.284%.

Case III: X and Y are in Logarithms
```{r}
# estimate the log-log model
LogLog_model <- lm(log(score) ~ log(income), data = CASchools)
# print robust coefficient summary to the console
coeftest(LogLog_model,
vcov = vcovHC, type = "HC1")
```
In a log-log model, a 1% change in X is associated with an estimated ˆ 1%
change in Y.

Plotting the log-lin and log-log models
```{r}
# generate a scatterplot
plot(log(score) ~ income,
col = "steelblue",
pch = 20,
data = CASchools,
main = "Log-Linear Regression Function")
# add the log-linear regression line
order_id <- order(CASchools$income)
lines(CASchools$income[order_id],
fitted(LogLinear_model)[order_id],
col = "red",
lwd = 2)
# add the log-log regression line
lines(sort(CASchools$income),
fitted(LogLog_model)[order(CASchools$income)],
col = "green",
lwd = 2)
# add a legend
legend("bottomright",
      legend = c("log-linear model", "log-log model"),
      lwd = 2,col = c("red", "green"))
```

Poly-log model

```{r}
# estimate the polylog model
polyLog_model <- lm(score ~ log(income) + I(log(income)^2) + I(log(income)^3),
data = CASchools)
# print robust summary to the console
coeftest(polyLog_model,
vcov = vcovHC, type = "HC1")
```
Comparing by ¯R2 we find that, leaving out the log-linear model, all models have
a similar adjusted fit. In the class of polynomial models, the cubic specification
has the highest ¯R2 whereas the linear-log specification is the best of the logmodels.

```{r}
# compute the adj. Rˆ2 for the nonlinear models
adj_R2 <-rbind("quadratic" = summary(quadratic_model)$adj.r.squared,
"cubic" = summary(cubic_model)$adj.r.squared,
"LinearLog" = summary(LinearLog_model)$adj.r.squared,
"LogLinear" = summary(LogLinear_model)$adj.r.squared,
"LogLog" = summary(LogLog_model)$adj.r.squared,
"polyLog" = summary(polyLog_model)$adj.r.squared)
# assign column names
colnames(adj_R2) <- "adj_R2"
print(adj_R2)
```
Let us now compare the cubic and the linear-log model by plotting the corresponding
estimated regression functions.

```{r}
# generate a scatterplot
plot(score ~ income,
data = CASchools,
col = "steelblue",
pch = 20,
main = "Linear-Log and Cubic Regression Functions")
# add the linear-log regression line
order_id <- order(CASchools$income)
lines(CASchools$income[order_id],
fitted(LinearLog_model)[order_id],
col = "darkgreen",
lwd = 2)
# add the cubic regression line
lines(x = CASchools$income[order_id],
y = fitted(cubic_model)[order_id],
col = "darkred",
lwd = 2)
```
Both regression lines look nearly identical. Altogether the linear-log model may
be preferable since it is more parsimonious in terms of regressors: it does not
include higher-degree polynomials.

8.3 Interactions Between Independent Variables
Method:
Compute expected values of Y for each possible set described by the set
of binary variables. Compare the expected values. The coefficients can
be expressed either as expected values or as the difference between at
least two expected values.

Model = Yi = B0 + B1 × D1i + B2 × D2i + B3 × (D1i × D2i) + ui 
where (D1i × D2i) is called an interaction term and B3 measures the difference in the
effect of having a college degree for women versus men.

Application to the Student-Teacher Ratio and the Percentage of English
Learners
```{r}
# append HiSTR to CASchools
CASchools$HiSTR <- as.numeric(CASchools$size >= 20)
# append HiEL to CASchools
CASchools$HiEL <- as.numeric(CASchools$english >= 10)
```
TestScore = B0 + B1 × HiSTR + B2 × HiEL + B3 × (HiSTR × HiEL) + ui.


```{r}
# estimate the model with a binary interaction term
bi_model <- lm(score ~ HiSTR * HiEL, data = CASchools)
# print a robust summary of the coefficients
coeftest(bi_model, vcov. = vcovHC, type = "HC1")
```
TestScore = 664.1− 1.9×HiSTR− 18.3×HiEL− 3.3×(HiSTR×HiEL)
it predicts that the effect of moving from a school district with a low studentteacher ratio to a district with a high student-teacher ratio, depending on high or low percentage of english learners is −1.9−3.3×HiEL. So for districts with a low share of english learners (HiEL = 0), the estimated effect is a decrease of 1.9 points in test scores while for districts with a large fraction of English learners


We can also use the model to estimate the mean test score for each possible
combination of the included binary variables.
```{r}
# 1.
predict(bi_model, newdata = data.frame("HiSTR" = 0, "HiEL" = 0))
# 2.
predict(bi_model, newdata = data.frame("HiSTR" = 0, "HiEL" = 1))
# 3.
predict(bi_model, newdata = data.frame("HiSTR" = 1, "HiEL" = 0))
# 4.
predict(bi_model, newdata = data.frame("HiSTR" = 1, "HiEL" = 1))
```
Interactions Between a Continuous and a Binary Variable
By adding the interaction term Xi × Di we allow the effect of an additional
year of work experience to differ between individuals with and without college
degree,
Yi = B0 + B1Xi + B2Di + B3(Xi × Di) + ui
Here, B3 is the expected difference in the effect of an additional year of work
experience for college graduates versus non-graduates. Another possible specification
is :
Yi = B0 + B1Xi + B2(Xi × Di) + ui.
This model states that the expected impact of an additional year of work experience
on earnings differs for college graduates and non-graduates but that
graduating on its own does not increase earnings.

```{r}
# generate artificial data
set.seed(1)
X <- runif(200,0, 15)
D <- sample(0:1, 200, replace = T)
Y <- 450 + 150 * X + 500 * D + 50 * (X * D) + rnorm(200, sd = 300)
# divide plotting area accordingly
m <- rbind(c(1, 2), c(3, 0))
graphics::layout(m)
# estimate the models and plot the regression lines
# 1. (baseline model)
plot(X, log(Y),
pch = 20,
col = "steelblue",
main = "Different Intercepts, Same Slope")
mod1_coef <- lm(log(Y) ~ X + D)$coefficients
abline(coef = c(mod1_coef[1], mod1_coef[2]),
col = "red",
lwd = 1.5)
abline(coef = c(mod1_coef[1] + mod1_coef[3], mod1_coef[2]),
col = "green",
lwd = 1.5)
# 2. (baseline model + interaction term)
plot(X, log(Y),
pch = 20,
col = "steelblue",
main = "Different Intercepts, Different Slopes")
mod2_coef <- lm(log(Y) ~ X + D + X:D)$coefficients
abline(coef = c(mod2_coef[1], mod2_coef[2]),
col = "red",
lwd = 1.5)
abline(coef = c(mod2_coef[1] + mod2_coef[3], mod2_coef[2] + mod2_coef[4]),
col = "green",
lwd = 1.5)
# 3. (omission of D as regressor + interaction term)
plot(X, log(Y),
pch = 20,
col = "steelblue",
main = "Same Intercept, Different Slopes")
mod3_coef <- lm(log(Y) ~ X + X:D)$coefficients
abline(coef = c(mod3_coef[1], mod3_coef[2]),
col = "red",
lwd = 1.5)
abline(coef = c(mod3_coef[1], mod3_coef[2] + mod3_coef[3]),
col = "green",
lwd = 1.5)
       
```
Application to the Student-Teacher Ratio and the Percentage of English
Learners
Using a model specification like the second one discussed in Key Concept 8.3
(different slope, different intercept) we may answer the question whether the
effect on test scores of decreasing the student-teacher ratio depends on whether
there are many or few English learners. We estimate the regression model

TestScorei = B0 + B1 × sizei + B2 × HiELi + B2(sizei × HiELi) + ui.

```{r}
# estimate the model
bci_model <- lm(score ~ size + HiEL + size * HiEL, data = CASchools)
# print robust summary of coefficients to the console
coeftest(bci_model, vcov. = vcovHC, type = "HC1")

```
The estimated regression model is
TestScore = 682.2− 0.97× size + 5.6× HiEL − 1.28× (size × HiEL).

The estimated regression line for districts with a low fraction of English learners
(HiELi = 0) is
TestScore = 682.2 − 0.97 × sizei.
For districts with a high fraction of English learners we have
TestScore = 682.2 + 5.6 − 0.97 × sizei − 1.28 × sizei =687.8 − 2.25 × sizei.

Interpretations :The predicted increase in test scores following a reduction of the student-teacher ratio by 1 unit is about 0.97 points in districts where the fraction of Englishlearners is low but 2.25 in districts with a high share of English learners. From the coefficient on the interaction term size × HiEL we see that the difference between both effects is 1.28 points.

```{r}
# identify observations with PctEL >= 10
id <- CASchools$english >= 10
# plot observations with HiEL = 0 as red dots
plot(CASchools$size[!id], CASchools$score[!id],
xlim = c(0, 27),
ylim = c(600, 720),
pch = 20,
col = "red",
main = "",
xlab = "Class Size",
ylab = "Test Score")
# plot observations with HiEL = 1 as green dots
points(CASchools$size[id], CASchools$score[id],
pch = 20,
col = "green")
# read out estimated coefficients of bci_model
coefs <- bci_model$coefficients
# draw the estimated regression line for HiEL = 0
abline(coef = c(coefs[1], coefs[2]),
col = "red",
lwd = 1.5)
# draw the estimated regression line for HiEL = 1
abline(coef = c(coefs[1] + coefs[3], coefs[2] + coefs[4]),
col = "green",
lwd = 1.5 )
# add a legend to the plot
legend("topright",
pch = c(20, 20),
col = c("red", "green"),
legend = c("HiEL = 0", "HiEL = 1"))
```
Interactions Between Two Continuous Variables
Application to the Student-Teacher Ratio and the Percentage
of English Learners
We now examine the interaction between the continuous variables studentteacher
ratio and the percentage of English learners.
```{r}
# estimate regression model including the interaction between 'PctEL' and 'size'
cci_model <- lm(score ~ size + english + english * size, data = CASchools)
# print a summary to the console
coeftest(cci_model, vcov. = vcovHC, type = "HC1")
```
The estimated model equation is
TestScore = 686.3− 1.12×STR− 0.67×PctEL+0.0012×(STR×PctEL).
For the interpretation, let us consider the quartiles of PctEL.

```{r}
summary(CASchools$english)

```
According to (8.2), if PctEL is at its median value of 8.78, the slope of the
regression function relating test scores and the student teacher ratio is predicted to be −1.12 + 0.0012 × 8.78 = −1.11. This means that increasing the studentteacher ratio by one unit is expected to deteriorate test scores by 1.11 points.For the 75% quantile, the estimated change on TestScore of a one-unit increase in STR is estimated by −1.12+0.0012×23.0 = −1.09 so the slope is somewhatlower. The interpretation is that for a school district with a share of 23% Englishlearners, a reduction of the student-teacher ratio by one unit is expected to increase test scores by only 1.09 points.
